<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Theoretical aspects of ML</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Theoretical aspects of ML</h1>
</header>
<hr />
<h2 id="deep-neural-networks">Deep Neural Networks</h2>
<h3 id="supervised-deep-learning">Supervised Deep Learning</h3>
<h4 id="representation">Representation</h4>
<ul>
<li>Deeper networks can represent functions with exponentially more “kinks”
<ul>
<li><a href="https://arxiv.org/abs/1402.1869">On the Number of Linear Regions of Deep Neural Networks</a></li>
<li><a href="http://proceedings.mlr.press/v49/telgarsky16.pdf">Benefits of depth in neural networks (PDF)</a></li>
</ul></li>
<li>There exist functions that can be represented efficiently with 3 layers, but not with 2 layers
<ul>
<li><a href="https://arxiv.org/abs/1512.03965">The Power of Depth for Feedforward Neural Networks</a></li>
</ul></li>
<li>Deep networks break curse of dimensionality
<ul>
<li><a href="https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-058-v6.pdf">Theory I: Why and When Can Deep Networks Avoid the Curse of Dimensionality? (PDF)</a></li>
</ul></li>
</ul>
<h4 id="optimisation">Optimisation</h4>
<ul>
<li>NP Hardness of learning even small networks.
<ul>
<li><a href="https://papers.nips.cc/paper/125-training-a-3-node-neural-network-is-np-complete.pdf">Training a 3-node Neural Network is NP-Complete (PDF)</a></li>
</ul></li>
<li>Hardness of learning parity like functions and deep networks
<ul>
<li><a href="https://core.ac.uk/download/pdf/62919210.pdf">Embedding Hard Learning Problems Into Gaussian Space (PDF)</a></li>
</ul></li>
<li>Failures of Gradient based methods
<ul>
<li><a href="https://arxiv.org/abs/1703.07950.abs">Failures of Gradient-Based Deep Learning</a></li>
</ul></li>
<li>Loss surface arguments
<ul>
<li><a href="http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf">Geometry of Neural Network Loss Surfaces via Random Matrix Theory (PDF)</a></li>
<li><a href="http://proceedings.mlr.press/v38/choromanska15.pdf">The Loss Surfaces of Multilayer Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1511.04210">On the Quality of the Initial Basin in Overspecified Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1703.09833abs">Theory II: Landscape of the Empirical Risk in Deep Learning</a></li>
<li><a href="http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf">Deep Learning without Poor Local Minima (PDF)</a></li>
</ul></li>
<li>Reparameterisation/Normalisation approaches:
<ul>
<li>Batch Norm: <a href="https://arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization?</a></li>
<li>Weight norm:</li>
<li>Layer Norm:</li>
<li>Path Norm : <a href="https://arxiv.org/abs/1506.02617">Path-SGD: Path-Normalized Optimization in Deep Neural Networks</a></li>
<li>Natural Gradient: <a href="https://arxiv.org/abs/1711.01530">Fisher-Rao Metric, Geometry, and Complexity of Neural Networks</a></li>
</ul></li>
<li>Weight sharing (ala CNNs) help in optimisation
<ul>
<li><a href="https://arxiv.org/abs/1802.02547">Learning One Convolutional Layer with Overlapping Patches</a></li>
<li><a href="https://arxiv.org/abs/1709.06129">When is Convolutional Filter Easy to Learn?</a></li>
</ul></li>
<li>Deep Linear Networks for understanding optimisation:
<ul>
<li><a href="https://arxiv.org/abs/1312.6120">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a></li>
<li><a href="http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf">Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global (PDF)</a></li>
</ul></li>
<li>Deep nets learn for linearly-separable and other types of “structured data” or in “PAC setting”
<ul>
<li><a href="https://arxiv.org/abs/1710.10174">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a></li>
<li><a href="https://arxiv.org/abs/1808.01204">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></li>
<li><a href="https://arxiv.org/abs/1712.00779">Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima</a></li>
</ul></li>
<li>Overparameterised deep nets “converge” (in training, that is)
<ul>
<li><a href="https://arxiv.org/abs/1811.03962">A Convergence Theory for Deep Learning via Over-Parameterization</a></li>
<li><a href="https://arxiv.org/abs/1809.10749">On the Loss Landscape of a Class of Deep Neural Networks with No Bad Local Valleys</a></li>
</ul></li>
<li>Stochastic Gradient Descent (SGD) seems to converge to the global minimum in a certain way
<ul>
<li><a href="https://arxiv.org/abs/1901.00451">SGD Converges to Global Minimum in Deep Learning via Star-Convex Path</a></li>
<li><a href="https://arxiv.org/abs/1811.02564">On exponential convergence of SGD in non-convex over-parametrized learning</a></li>
</ul></li>
<li>In contrast, Gradient Descent (as opposed to SGD) seems to converge along the shortest path instead of a star-convex path
<ul>
<li><a href="https://arxiv.org/abs/1812.10004">Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path</a></li>
<li><a href="https://arxiv.org/abs/1811.03804">Gradient Descent Finds Global Minima of Deep Neural Networks</a></li>
</ul></li>
<li>Using ResNets instead of standard deep nets help in optimisation
<ul>
<li>TO-FILL</li>
</ul></li>
</ul>
<h4 id="generalisation">Generalisation</h4>
<ul>
<li>The paper that drove home the incompleteness of our understanding about how deep networks generalize (ICLR 2017 Best Paper)
<ul>
<li><a href="https://arxiv.org/abs/1611.03530">Understanding Deep Learning Requires Rethinking Generalization</a></li>
</ul></li>
<li>Once the above paper came out, the same properties were observed in “overfitted” kernel models as well
<ul>
<li><a href="https://arxiv.org/abs/1802.01396">To understand deep learning we need to understand kernel learning</a></li>
</ul></li>
<li>Size of weights, not number of parameters defines complexity
<ul>
<li><a href="http://www.yaroslavvb.com/papers/bartlett-sample.pdf">The Sample Complexity of Pattern Classification with Neural Networks: The size of the weights is more important than the size of the network (PDF)</a></li>
</ul></li>
<li>Contrast the above paper with this one that links generalization ability with the number of parameters in a massively overparameterized deep neural network
<ul>
<li><a href="https://arxiv.org/abs/1901.01608">Scaling description of generalization with number of parameters in deep learning</a></li>
</ul></li>
<li>VC dimension of Neural Nets.
<ul>
<li><a href="http://mathsci.kaist.ac.kr/~nipl/mas557/VCD_ANN_3.pdf">VC Dimension of Neural Networks (PDF)</a></li>
</ul></li>
<li>Spectral Normalised Margin for deep networks.
<ul>
<li><a href="https://arxiv.org/abs/1706.08498">Spectrally-normalized margin bounds for neural networks</a></li>
</ul></li>
<li>Compression properties of deep networks.
<ul>
<li><a href="https://arxiv.org/abs/1802.05296">Stronger generalization bounds for deep nets via a compression approach</a></li>
</ul></li>
</ul>
<h4 id="overparametrization-reconciling-coexistence-of-zero-training-error-and-generalization">Overparametrization: Reconciling coexistence of zero training error and generalization</h4>
<p>The coexistence of zero training error (usually implying “overtraining” for classical ML models) with low test error (implying successful generalization) for massively overparameterized deep neural networks is the central theoretical mystery. This section lists a few papers that address this issue.</p>
<ul>
<li>Clearly, training error is a poor indicator of test error, given the above observations. Is it possible to modify training error to make it a more accurate indicator of test error?
<ul>
<li><a href="https://arxiv.org/abs/1807.09659">A Surprising Linear Relationship Predicts Test Performance in Deep Networks</a></li>
<li><a href="https://arxiv.org/abs/1810.00113">Predicting the Generalization Gap in Deep Networks with Margin Distributions</a></li>
</ul></li>
<li>The loss function during training (for classification) is often cross-entropy, and it turns out that this particular loss function has desirable properties as it is a form of Lipschitz regularization
<ul>
<li><a href="https://arxiv.org/abs/1808.09540">Lipschitz Regularized Deep Neural Networks Converge and Generalize</a></li>
</ul></li>
<li>These papers say that trained deep learning models are biased toward simple functions
<ul>
<li><a href="https://arxiv.org/abs/1706.10239">Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes</a></li>
<li><a href="https://arxiv.org/abs/1805.08522">Deep Learning Generalizes Because the Parameter-Function Map is Biased Towards Simple Functions</a></li>
</ul></li>
<li>Deep neural networks have better generalization ability than kernel-based methods on tasks such as image classification. This paper purports to explain why.
<ul>
<li><a href="https://arxiv.org/abs/1810.05369">On the Margin Theory of Feedforward Neural Networks</a></li>
</ul></li>
<li>This paper says that we just haven’t explored massively over-parameterized models before, and that they all (deep neural networks and other machine learning models too) exhibit the same behavior:
<ul>
<li><a href="https://arxiv.org/abs/1812.11118">Reconciling modern machine learning and the bias-variance trade-off</a></li>
</ul></li>
</ul>
<h4 id="other-paradigms-for-understanding-deep-networks">Other Paradigms for Understanding Deep Networks</h4>
<ul>
<li>Information Bottleneck
<ul>
<li><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a></li>
</ul></li>
<li>Differential Topology yields some insights into the properties of deep feedforward neural networks
<ul>
<li><a href="https://arxiv.org/abs/1811.10304">A Differential Topological View of Challenges in Learning with Feedforward Neural Networks</a></li>
</ul></li>
<li>Random weights
<ul>
<li><a href="https://arxiv.org/abs/1504.08291">Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?</a></li>
</ul></li>
<li>Sum-Product networks
<ul>
<li><a href="https://arxiv.org/abs/1705.02302">Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions</a></li>
<li><a href="https://papers.nips.cc/paper/4350-shallow-vs-deep-sum-product-networks.pdf">Shallow vs. Deep Sum-Product Networks (PDF)</a></li>
</ul></li>
<li>Convolutional nets learning filters/scattering networks/sparse coding for images
<ul>
<li><a href="https://arxiv.org/abs/1512.06293">A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction</a></li>
<li><a href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf">Convolutional Kernel Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1601.04920">Understanding Deep Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1203.1513">Invariant Scattering Convolution Networks</a></li>
</ul></li>
<li>Improper learning using Kernels
<ul>
<li><a href="https://arxiv.org/abs/1510.03528"><span class="math inline">ℓ<sub>1</sub></span>-regularized Neural Networks are Improperly Learnable in Polynomial Time</a></li>
</ul></li>
<li>SGD/Architecture/Initialisation together to define a “hypothesis class”
<ul>
<li><a href="https://arxiv.org/abs/1702.08503">SGD Learns the Conjugate Kernel Class of the Network</a></li>
<li><a href="https://arxiv.org/abs/1602.05897">Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity</a></li>
</ul></li>
<li>“Convexified” neural networks
<ul>
<li><a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1609.01000">Convexified Convolutional Neural Networks</a></li>
</ul></li>
</ul>
<hr />
</body>
</html>
