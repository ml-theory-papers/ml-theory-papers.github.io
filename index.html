<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Theoretical aspects of ML</title>
  <style type="text/css">code{white-space: pre;}</style>
  <meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
</head>
<body>
<div id="header">
<h1 class="title">Theoretical aspects of ML</h1>
</div>
<hr />
<h2 id="deep-neural-networks">Deep Neural Networks</h2>
<h3 id="supervised-deep-learning">Supervised Deep Learning</h3>
<h4 id="representation">Representation</h4>
<ul>
<li>Deeper networks can represent functions with exponentially more &quot;kinks&quot;</li>
<li><a href="https://arxiv.org/abs/1402.1869">On the Number of Linear Regions of Deep Neural Networks</a></li>
<li><a href="http://proceedings.mlr.press/v49/telgarsky16.pdf">Benefits of depth in neural networks (PDF)</a></li>
<li>There exist functions that can be represented efficiently with 3 layers, but not with 2 layers</li>
<li><a href="https://arxiv.org/abs/1512.03965">The Power of Depth for Feedforward Neural Networks</a></li>
<li>Deep networks break curse of dimensionality</li>
<li><a href="https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-058-v6.pdf">Theory I: Why and When Can Deep Networks Avoid the Curse of Dimensionality? (PDF)</a></li>
</ul>
<h4 id="optimisation">Optimisation</h4>
<ul>
<li>NP Hardness of learning even small networks.</li>
<li><a href="https://papers.nips.cc/paper/125-training-a-3-node-neural-network-is-np-complete.pdf">Training a 3-node Neural Network is NP-Complete (PDF)</a></li>
<li>Hardness of learning parity like functions and deep networks</li>
<li><a href="https://core.ac.uk/download/pdf/62919210.pdf">Embedding Hard Learning Problems Into Gaussian Space (PDF)</a></li>
<li>Failures of Gradient based methods</li>
<li><a href="https://arxiv.org/abs/1703.07950.abs">Failures of Gradient-Based Deep Learning</a></li>
<li>Loss surface arguments</li>
<li><a href="http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf">Geometry of Neural Network Loss Surfaces via Random Matrix Theory (PDF)</a></li>
<li><a href="http://proceedings.mlr.press/v38/choromanska15.pdf">The Loss Surfaces of Multilayer Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1511.04210">On the Quality of the Initial Basin in Overspecified Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1703.09833abs">Theory II: Landscape of the Empirical Risk in Deep Learning</a></li>
<li><a href="http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf">Deep Learning without Poor Local Minima (PDF)</a></li>
<li>Reparameterisation/Normalisation approaches:</li>
<li>Batch Norm: <a href="https://arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization?</a></li>
<li>Weight norm:</li>
<li>Layer Norm:</li>
<li>Path Norm : <a href="https://arxiv.org/abs/1506.02617">Path-SGD: Path-Normalized Optimization in Deep Neural Networks</a></li>
<li>Natural Gradient: <a href="https://arxiv.org/abs/1711.01530">Fisher-Rao Metric, Geometry, and Complexity of Neural Networks</a></li>
<li>Weight sharing (ala CNNs) help in optimisation</li>
<li><a href="https://arxiv.org/abs/1802.02547">Learning One Convolutional Layer with Overlapping Patches</a></li>
<li><a href="https://arxiv.org/abs/1709.06129">When is Convolutional Filter Easy to Learn?</a></li>
<li>Deep Linear Networks for understanding optimisation:</li>
<li><a href="https://arxiv.org/abs/1312.6120">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a></li>
<li><a href="http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf">Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global (PDF)</a></li>
<li>Deep nets learn for linearly-separable and other types of “structured data” or in “PAC setting”</li>
<li><a href="https://arxiv.org/abs/1710.10174">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a></li>
<li><a href="https://arxiv.org/abs/1808.01204">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></li>
<li><a href="https://arxiv.org/abs/1712.00779">Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima</a></li>
<li>Overparameterised deep nets “converge” (in training, that is)</li>
<li><a href="https://arxiv.org/abs/1811.03962">A Convergence Theory for Deep Learning via Over-Parameterization</a></li>
<li><a href="https://arxiv.org/abs/1809.10749">On the Loss Landscape of a Class of Deep Neural Networks with No Bad Local Valleys</a></li>
<li>Stochastic Gradient Descent (SGD) seems to converge to the global minimum in a certain way</li>
<li><a href="https://arxiv.org/abs/1901.00451">SGD Converges to Global Minimum in Deep Learning via Star-Convex Path</a></li>
<li><a href="https://arxiv.org/abs/1811.02564">On exponential convergence of SGD in non-convex over-parametrized learning</a></li>
<li>In contrast, Gradient Descent (as opposed to SGD) seems to converge along the shortest path instead of a star-convex path</li>
<li><a href="https://arxiv.org/abs/1812.10004">Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path</a></li>
<li><a href="https://arxiv.org/abs/1811.03804">Gradient Descent Finds Global Minima of Deep Neural Networks</a></li>
<li>Using ResNets instead of standard deep nets help in optimisation</li>
<li>TO-FILL</li>
</ul>
<h4 id="generalisation">Generalisation</h4>
<ul>
<li>The paper that drove home the incompleteness of our understanding about how deep networks generalize (ICLR 2017 Best Paper)</li>
<li><a href="https://arxiv.org/abs/1611.03530">Understanding Deep Learning Requires Rethinking Generalization</a></li>
<li>Once the above paper came out, the same properties were observed in &quot;overfitted&quot; kernel models as well</li>
<li><a href="https://arxiv.org/abs/1802.01396">To understand deep learning we need to understand kernel learning</a></li>
<li>Size of weights, not number of parameters defines complexity</li>
<li><a href="http://www.yaroslavvb.com/papers/bartlett-sample.pdf">The Sample Complexity of Pattern Classification with Neural Networks: The size of the weights is more important than the size of the network (PDF)</a></li>
<li>Contrast the above paper with this one that links generalization ability with the number of parameters in a massively overparameterized deep neural network</li>
<li><a href="https://arxiv.org/abs/1901.01608">Scaling description of generalization with number of parameters in deep learning</a></li>
<li>VC dimension of Neural Nets.</li>
<li><a href="http://mathsci.kaist.ac.kr/~nipl/mas557/VCD_ANN_3.pdf">VC Dimension of Neural Networks (PDF)</a></li>
<li>Spectral Normalised Margin for deep networks.</li>
<li><a href="https://arxiv.org/abs/1706.08498">Spectrally-normalized margin bounds for neural networks</a></li>
<li>Compression properties of deep networks.</li>
<li><a href="https://arxiv.org/abs/1802.05296">Stronger generalization bounds for deep nets via a compression approach</a></li>
</ul>
<h4 id="overparametrization-reconciling-coexistence-of-zero-training-error-and-generalization">Overparametrization: Reconciling coexistence of zero training error and generalization</h4>
<p>The coexistence of zero training error (usually implying &quot;overtraining&quot; for classical ML models) with low test error (implying successful generalization) for massively overparameterized deep neural networks is the central theoretical mystery. This section lists a few papers that address this issue.</p>
<ul>
<li>Clearly, training error is a poor indicator of test error, given the above observations. Is it possible to modify training error to make it a more accurate indicator of test error?</li>
<li><a href="https://arxiv.org/abs/1807.09659">A Surprising Linear Relationship Predicts Test Performance in Deep Networks</a></li>
<li><a href="https://arxiv.org/abs/1810.00113">Predicting the Generalization Gap in Deep Networks with Margin Distributions</a></li>
<li>The loss function during training (for classification) is often cross-entropy, and it turns out that this particular loss function has desirable properties as it is a form of Lipschitz regularization</li>
<li><a href="https://arxiv.org/abs/1808.09540">Lipschitz Regularized Deep Neural Networks Converge and Generalize</a></li>
<li>These papers say that trained deep learning models are biased toward simple functions</li>
<li><a href="https://arxiv.org/abs/1706.10239">Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes</a></li>
<li><a href="https://arxiv.org/abs/1805.08522">Deep Learning Generalizes Because the Parameter-Function Map is Biased Towards Simple Functions</a></li>
<li>Deep neural networks have better generalization ability than kernel-based methods on tasks such as image classification. This paper purports to explain why.</li>
<li><a href="https://arxiv.org/abs/1810.05369">On the Margin Theory of Feedforward Neural Networks</a> This paper by Sanjeev Arora et al. has a comprehensive explanation of generalizability in a neural network, and emphasizes the fact that the nature of the data has something to do with generalization ability of a trained model (because a deep learning model can learn scrambled labels with zero error too, but such a model cannot generalize). Unfortunately, the proofs here are for: (a) GD and not SGD; (b) a two-layer network only (so not really &quot;deep&quot;); and (c) ReLU activations only (though this isn't a serious limitation). Before you begin reading this paper, note that the next one below proves generalizability of a GD-trained overparameterized multi-layer (more than two) network and hence supersedes the Arora et al. paper, but the Arora et al. paper is still useful reading for its insights that the nature of the data itself must determine whether generalization is possible.</li>
<li><a href="https://arxiv.org/abs/1901.08584">Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks</a></li>
<li>As stated in the discussion of the Arora et al. paper above, this February 2019 paper by Cao and Gu generalizes the Arora et al. results to more than two layers, though still for GD and ReLU only. The ReLU restriction is minor, so all that remains is for the extension of these results to SGD instead of GD.</li>
<li><a href="https://arxiv.org/abs/1902.01384">A Generalization Theory of Gradient Descent for Learning Over-Parameterized Deep ReLU Networks</a></li>
<li>This paper by Belkin et al. says that we just haven't explored massively over-parameterized models before, and that they all (deep neural networks and other machine learning models too) exhibit the same behavior:</li>
<li><a href="https://arxiv.org/abs/1812.11118">Reconciling modern machine learning and the bias-variance trade-off</a></li>
<li>After Belkin et al. showed that the &quot;double-descent&quot; shape of the plot of model complexity versus test set error could be seen on kernel-based models in addition to neural networks, researchers began to re-examine &quot;classical&quot; machine learning models to see if they could reproduce this &quot;double-descent&quot; behavior with simpler models that could be investigated on a laptop in minutes rather than on a data center in days or weeks. Two recent papers have shown this by moving from classification to regression, specifically, linear regression. In the regression setting, getting to zero training set error is called interpolation, and the absence of a regularizer in the objective function for training is called &quot;ridgeless.&quot; Sahai et al. discovered double-descent behavior in a nearly-trivial case of polynomial fitting on a training set of size just 9:</li>
<li><a href="https://arxiv.org/abs/1903.09139">Harmless interpolation of noisy data in regression</a> A comprehensive explanation of the different regions of the double-descent plot, and how they arise in a ridgeless interpolative model, was shown almost at the same time as Sahai et al. by the redoubtable Stanford team of Hastie et al. for a 2-layer neural network:</li>
<li><a href="https://arxiv.org/abs/1903.08560">Surprises in High-Dimensional Ridgeless Least Squares Interpolation</a> At the present time (March 2019), the above two regression-based papers yield the most insight into how generalization is possible along with interpolation in a (albeit shallow) neural network, as opposed to the mathematical &quot;existence proof&quot; approach in the classification-based papers by Arora et al. and Cao &amp; Gu.</li>
</ul>
<h4 id="other-paradigms-for-understanding-deep-networks">Other Paradigms for Understanding Deep Networks</h4>
<ul>
<li>Information Bottleneck</li>
<li><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a></li>
<li>Differential Topology yields some insights into the properties of deep feedforward neural networks</li>
<li><a href="https://arxiv.org/abs/1811.10304">A Differential Topological View of Challenges in Learning with Feedforward Neural Networks</a></li>
<li>Random weights</li>
<li><a href="https://arxiv.org/abs/1504.08291">Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?</a></li>
<li>Sum-Product networks</li>
<li><a href="https://arxiv.org/abs/1705.02302">Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions</a></li>
<li><a href="https://papers.nips.cc/paper/4350-shallow-vs-deep-sum-product-networks.pdf">Shallow vs. Deep Sum-Product Networks (PDF)</a></li>
<li>Convolutional nets learning filters/scattering networks/sparse coding for images</li>
<li><a href="https://arxiv.org/abs/1512.06293">A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction</a></li>
<li><a href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf">Convolutional Kernel Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1601.04920">Understanding Deep Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1203.1513">Invariant Scattering Convolution Networks</a></li>
<li>Improper learning using Kernels</li>
<li><a href="https://arxiv.org/abs/1510.03528"><span class="math inline">ℓ<sub>1</sub></span>-regularized Neural Networks are Improperly Learnable in Polynomial Time</a></li>
<li>SGD/Architecture/Initialisation together to define a “hypothesis class”</li>
<li><a href="https://arxiv.org/abs/1702.08503">SGD Learns the Conjugate Kernel Class of the Network</a></li>
<li><a href="https://arxiv.org/abs/1602.05897">Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity</a></li>
<li>“Convexified” neural networks</li>
<li><a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1609.01000">Convexified Convolutional Neural Networks</a></li>
</ul>
<hr />
</body>
</html>
