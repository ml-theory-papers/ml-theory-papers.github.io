<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Theoretical aspects of ML</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <meta name="viewport" content="initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Theoretical aspects of ML</h1>
</header>
<hr />
<h2 id="deep-neural-networks">Deep Neural Networks</h2>
<h3 id="supervised-deep-learning">Supervised Deep Learning</h3>
<h4 id="representation">Representation</h4>
<ul>
<li>Deeper networks can represent functions with exponentially more “kinks”
<ul>
<li><a href="https://arxiv.org/abs/1402.1869">On the Number of Linear Regions of Deep Neural Networks</a></li>
<li><a href="http://proceedings.mlr.press/v49/telgarsky16.pdf">Benefits of depth in neural networks (PDF)</a></li>
</ul></li>
<li>There exist functions that can be represented efficiently with 3 layers, but not with 2 layers
<ul>
<li><a href="https://arxiv.org/abs/1512.03965">The Power of Depth for Feedforward Neural Networks</a></li>
</ul></li>
<li>Deep networks break curse of dimensionality
<ul>
<li><a href="https://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-058-v6.pdf">Theory I: Why and When Can Deep Networks Avoid the Curse of Dimensionality? (PDF)</a></li>
</ul></li>
</ul>
<h4 id="optimisation">Optimisation</h4>
<ul>
<li>NP Hardness of learning even small networks.
<ul>
<li><a href="https://papers.nips.cc/paper/125-training-a-3-node-neural-network-is-np-complete.pdf">Training a 3-node Neural Network is NP-Complete (PDF)</a></li>
</ul></li>
<li>Hardness of learning parity like functions and deep networks
<ul>
<li><a href="https://core.ac.uk/download/pdf/62919210.pdf">Embedding Hard Learning Problems Into Gaussian Space (PDF)</a></li>
</ul></li>
<li>Failures of Gradient based methods
<ul>
<li><a href="https://arxiv.org/abs/1703.07950.abs">Failures of Gradient-Based Deep Learning</a></li>
</ul></li>
<li>Loss surface arguments
<ul>
<li><a href="http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf">Geometry of Neural Network Loss Surfaces via Random Matrix Theory (PDF)</a></li>
<li><a href="http://proceedings.mlr.press/v38/choromanska15.pdf">The Loss Surfaces of Multilayer Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1511.04210">On the Quality of the Initial Basin in Overspecified Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1703.09833abs">Theory II: Landscape of the Empirical Risk in Deep Learning</a></li>
<li><a href="http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf">Deep Learning without Poor Local Minima (PDF)</a></li>
</ul></li>
<li>Reparameterisation/Normalisation approaches:
<ul>
<li>Batch Norm: <a href="https://arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization?</a></li>
<li>Weight norm:</li>
<li>Layer Norm:</li>
<li>Path Norm : <a href="https://arxiv.org/abs/1506.02617">Path-SGD: Path-Normalized Optimization in Deep Neural Networks</a></li>
<li>Natural Gradient: <a href="https://arxiv.org/abs/1711.01530">Fisher-Rao Metric, Geometry, and Complexity of Neural Networks</a></li>
</ul></li>
<li>Weight sharing (ala CNNs) help in optimisation
<ul>
<li><a href="https://arxiv.org/abs/1802.02547">Learning One Convolutional Layer with Overlapping Patches</a></li>
<li><a href="https://arxiv.org/abs/1709.06129">When is Convolutional Filter Easy to Learn?</a></li>
</ul></li>
<li>Deep Linear Networks for understanding optimisation:
<ul>
<li><a href="https://arxiv.org/abs/1312.6120">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a></li>
<li><a href="http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf">Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global (PDF)</a></li>
</ul></li>
<li>Deep nets learn for linearly-separable and other types of “structured data” or in “PAC setting”
<ul>
<li><a href="https://arxiv.org/abs/1710.10174">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a></li>
<li><a href="https://arxiv.org/abs/1808.01204">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a></li>
<li><a href="https://arxiv.org/abs/1712.00779">Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima</a></li>
</ul></li>
<li>Overparameterised deep nets “converge”
<ul>
<li><a href="https://arxiv.org/abs/1811.03962">A Convergence Theory for Deep Learning via Over-Parameterization</a></li>
</ul></li>
<li>Using ResNets instead of standard deep nets help in optimisation
<ul>
<li>TO-FILL</li>
</ul></li>
</ul>
<h4 id="generalisation">Generalisation</h4>
<ul>
<li>Size of weights, not number of parameters defines complexity
<ul>
<li><a href="http://www.yaroslavvb.com/papers/bartlett-sample.pdf">The Sample Complexity of Pattern Classification with Neural Networks: The size of the weights is more important than the size of the network (PDF)</a></li>
</ul></li>
<li>VC dimension of Neural Nets.
<ul>
<li><a href="http://mathsci.kaist.ac.kr/~nipl/mas557/VCD_ANN_3.pdf">VC Dimension of Neural Networks (PDF)</a></li>
</ul></li>
<li>Spectral Normalised Margin for deep networks.
<ul>
<li><a href="https://arxiv.org/abs/1706.08498">Spectrally-normalized margin bounds for neural networks</a></li>
</ul></li>
<li>Compression properties of deep networks.
<ul>
<li><a href="https://arxiv.org/abs/1802.05296">Stronger generalization bounds for deep nets via a compression approach</a></li>
</ul></li>
</ul>
<h4 id="other-paradigms-for-understanding-deep-networks">Other Paradigms for Understanding Deep Networks</h4>
<ul>
<li>Information Bottleneck
<ul>
<li><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a></li>
</ul></li>
<li>Random weights
<ul>
<li><a href="https://arxiv.org/abs/1504.08291">Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?</a></li>
</ul></li>
<li>Sum-Product networks
<ul>
<li><a href="https://arxiv.org/abs/1705.02302">Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions</a></li>
<li><a href="https://papers.nips.cc/paper/4350-shallow-vs-deep-sum-product-networks.pdf">Shallow vs. Deep Sum-Product Networks (PDF)</a></li>
</ul></li>
<li>Convolutional nets learning filters/scattering networks/sparse coding for images
<ul>
<li><a href="https://arxiv.org/abs/1512.06293">A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction</a></li>
<li><a href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf">Convolutional Kernel Networks (PDF)</a></li>
<li><a href="https://arxiv.org/abs/1601.04920">Understanding Deep Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1203.1513">Invariant Scattering Convolution Networks</a></li>
</ul></li>
<li>Improper learning using Kernels
<ul>
<li><a href="https://arxiv.org/abs/1510.03528"><span class="math inline">ℓ<sub>1</sub></span>-regularized Neural Networks are Improperly Learnable in Polynomial Time</a></li>
</ul></li>
<li>SGD/Architecture/Initialisation together to define a “hypothesis class”
<ul>
<li><a href="https://arxiv.org/abs/1702.08503">SGD Learns the Conjugate Kernel Class of the Network</a></li>
<li><a href="https://arxiv.org/abs/1602.05897">Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity</a></li>
</ul></li>
<li>“Convexified” neural networks
<ul>
<li><a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the Curse of Dimensionality with Convex Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1609.01000">Convexified Convolutional Neural Networks</a></li>
</ul></li>
</ul>
<hr />
</body>
</html>
